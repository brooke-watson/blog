---
title: 'Natural Love Language Processing: LSTM Recurrent Neural Networks with Keras in R'
author: Brooke Watson
date: '2018-03-07'
slug: neural-nets-in-R
categories: []
output:
  blogdown::html_page:
    toc: true 
tags:
  - NLP
  - neural networks
  - deep learning
  - machine learning 
  - TensorFlow
  - Keras
  - R
  - LSTM
  - RNN
  - SQL
  - RSQLite 
description: "Training a Long Short-term Memory Recurrent Neural Network on text message data."
---


<div id="TOC">
<ul>
<li><a href="#getting-the-data">Getting the data</a><ul>
<li><a href="#reading-imessages-into-r">Reading iMessages into R</a></li>
</ul></li>
<li><a href="#preparing-text-data-for-an-lstm-neural-network">Preparing text data for an LSTM neural network</a></li>
<li><a href="#defining-a-keras-model">Defining a keras model</a></li>
<li><a href="#defining-model-output">Defining model output</a></li>
<li><a href="#training-the-model">Training the model</a></li>
<li><a href="#lstm-text-output">LSTM text output</a></li>
<li><a href="#measuring-performance">Measuring performance</a><ul>
<li><a href="#whats-happening-here">Whatâ€™s happening here?</a></li>
</ul></li>
<li><a href="#iterating">Iterating</a></li>
<li><a href="#archetypes">Archetypes</a></li>
<li><a href="#takeaways">Takeaways</a></li>
</ul>
</div>

<style>
  div.container {
    display:inline-block;
  }

</style>
<p>A few weeks ago, I saw JJ Allaireâ€™s <a href="https://www.youtube.com/watch?v=atiYXm7JZv0">talk</a> at the New York Open Statistical society meetup. JJ is the CEO of RStudio and one of the developers of Râ€™s <a href="https://keras.rstudio.com/">Keras</a> package, which interfaces with the Keras TensorFlow libraries from within R. His take on the interplay between deep learning and Râ€™s more traditional stats wheelhouse is thoughtful, straightforward, and refreshingly hype-free.</p>
<p>JJâ€™s talk gave me both inspiration and <a href="https://keras.rstudio.com/articles/examples/lstm_text_generation.html">vignettes</a> I could use to get started. It also happened to fall on the day after Valentineâ€™s Day.</p>
<p>The Keras vignette trains a model to write like <a href="https://keras.rstudio.com/articles/examples/lstm_text_generation.html">Nietzsche</a>. I trained mine to text like my fiancÃ© and me.</p>
<pre class="r"><code>library(keras)
library(RSQLite)
library(tidyverse)
library(tokenizers)
library(readxl)
library(here)
library(janitor)
library(lubridate)
library(utf8)</code></pre>
<div id="getting-the-data" class="section level1">
<h1>Getting the data</h1>
<p>When we first started dating, my fiancÃ© was living in New York while I was in Sydney, Australia. Naturally, we texted a <strong>lot</strong>. Fortunately, we did most of that texting in Viber, a Whatsapp-like international message service that lets you export your entire message history into a convenient .xlsx file. At some point, we saved these, so reading those into R was cake.</p>
<pre class="r"><code>library(readxl)
viber &lt;- read_excel(&quot;~/Documents/viber/data/Brooke Watson Viber Messages - Aussie Second Half.xlsx&quot;, col_names = FALSE) %&gt;% 
  remove_empty_rows()

# set column names 
names(viber) = c(&quot;date&quot;, &quot;time&quot;, &quot;person&quot;, &quot;number&quot;, &quot;message&quot;)

paste(&quot;Number of messages:&quot;, length(viber$message))
paste(&quot;Number of characters:&quot;, sum(nchar(viber$message), na.rm = TRUE))</code></pre>
<pre class="text"><code>[1] &quot;Number of messages: 2558&quot;
[1] &quot;Number of characters: 113701&quot;</code></pre>
<p>(Iâ€™m not including the raw data here, for obvious reasons. Are you insane? Not everything is reproducible, fam.)</p>
<p>Unfortunately, we didnâ€™t export this Excel this early enough. Both of us got new phones during the year or so that I lived in Oz, so we have roughly 2 months of our early conversations. 113701 characters is <em>decent</em>, but I wanted more.</p>
<div id="reading-imessages-into-r" class="section level2">
<h2>Reading iMessages into R</h2>
<p>We also have lots of iMessage data, which at first glance didnâ€™t seem to be exportable in any spreadsheet format. Fortunately, though, the Messages app on Macbook saves all chat history in a SQLite database that is stored in <code>~/Library/Messages/chat.db</code>. This means I could use RSQLite and SQL code to pull message data down into a tidy data table. (Many thanks to Steven Morse for a great play-by-play <a href="https://stmorse.github.io/journal/iMessage.html">post</a> on analyzing iMessage conversations.)</p>
<pre class="r"><code># find the path to the iMessages saved in the Message app  
filename &lt;- &quot;~/Library/Messages/chat.db&quot; 
# connect to the database 
con &lt;- dbConnect(drv = dbDriver(&quot;SQLite&quot;),
                dbname = filename)

# get a list of all tables
dbListTables(con)

# explore the handles table to find the chat id of interest 
handles = dbGetQuery(con, &quot;SELECT * from handle&quot;)

# use SQL to join the chat table (which holds the individual phone)
# to the actual messages 
imessage &lt;- dbGetQuery( con,&quot;SELECT ROWID, text, handle_id, is_from_me, date 
FROM message T1 
INNER JOIN chat_message_join T2 
    ON T2.chat_id=5 
    AND T1.ROWID=T2.message_id 
ORDER BY T1.date;&quot; )</code></pre>
<p>Now that Iâ€™ve scraped together all of the texts I could find, I need to bind them into one long text string. I dropped everything but the actual texts - I wonâ€™t use the dates, IDs, or other information as inputs into this model, so they can go.</p>
<pre class="r"><code>messages &lt;- bind_rows(
  data_frame(msg = viber$message), 
  data_frame(msg = imessage$text)
)</code></pre>
<p>Now I can remove all NAs, collapse, and tokenize the textâ€¦</p>
<pre class="r"><code>text &lt;- messages$msg[is.na(messages$msg) == FALSE] %&gt;% 
  str_to_lower() %&gt;%
  str_c(collapse = &quot;\n&quot;) %&gt;%
  tokenize_characters(strip_non_alphanum = FALSE, simplify = TRUE)
paste(&quot;Number of characters:&quot;, length(text))</code></pre>
<pre class="text"><code>[1] &quot;Number of characters: 265924&quot;</code></pre>
<p>â€¦and get a list of all the unique characters in the corpus.</p>
<pre class="r"><code>chars &lt;- text %&gt;%
  unique() %&gt;%
  sort()
paste(&quot;total chars:&quot;, length(chars))</code></pre>
<pre class="text"><code>[1] &quot;total chars: 176&quot;</code></pre>
<p>176?? Thatâ€™s way more characters than there are in the alphabet. Even including all digits and punctuation marks, this seems high. Letâ€™s see whatâ€™s in there:</p>
<pre class="r"><code>utf8::utf8_print(chars)</code></pre>
<pre class="text"><code>  [1] &quot;\n&quot;     &quot; &quot;      &quot;ã€€&quot;     &quot;_&quot;      &quot;ï¼¿&quot;     &quot;-&quot;      &quot;,&quot;     
  [8] &quot;;&quot;      &quot;:&quot;      &quot;!&quot;      &quot;Â¡&quot;      &quot;?&quot;      &quot;Â¿&quot;      &quot;.&quot;     
 [15] &quot;&#39;&quot;      &quot;â€˜&quot;      &quot;â€™&quot;      &quot;\&quot;&quot;     &quot;â€œ&quot;      &quot;â€&quot;      &quot;(&quot;     
 [22] &quot;)&quot;      &quot;[&quot;      &quot;]&quot;      &quot;@&quot;      &quot;*&quot;      &quot;/&quot;      &quot;\\&quot;    
 [29] &quot;&amp;&quot;      &quot;#&quot;      &quot;%&quot;      &quot;^&quot;      &quot;Â¯&quot;      &quot;+&quot;      &quot;&lt;&quot;     
 [36] &quot;=&quot;      &quot;&gt;&quot;      &quot;|&quot;      &quot;~&quot;      &quot;â”€&quot;      &quot;â”‚&quot;      &quot;â”Œ&quot;     
 [43] &quot;â”&quot;      &quot;â””&quot;      &quot;â”˜&quot;      &quot;â– &quot;      &quot;â˜º&quot;     &quot;â™€&quot;      &quot;âš¾&quot;    
 [50] &quot;ğŸ‡µğŸ‡¦&quot;     &quot;âœˆ&quot;     &quot;âœ¨&quot;     &quot;â¤&quot;     &quot;ğŸŒ&quot;     &quot;ğŸ¯&quot;     &quot;ğŸ¹&quot;    
 [57] &quot;ğŸ‰&quot;     &quot;ğŸ &quot;     &quot;ğŸ¼&quot;     &quot;ğŸ½&quot;     &quot;ğŸ¿&quot;     &quot;ğŸ˜&quot;     &quot;ğŸ‘€&quot;    
 [64] &quot;ğŸ‘‹&quot;     &quot;ğŸ‘Œ&quot;     &quot;ğŸ‘&quot;     &quot;ğŸ‘&quot;     &quot;ğŸ’€&quot;     &quot;ğŸ’•&quot;     &quot;ğŸ’¡&quot;    
 [71] &quot;ğŸ’¯&quot;     &quot;ğŸ”¥&quot;     &quot;ğŸ¤”&quot;     &quot;ğŸ˜&quot;     &quot;ğŸ˜‚&quot;     &quot;ğŸ˜„&quot;     &quot;ğŸ˜‡&quot;    
 [78] &quot;ğŸ˜‰&quot;     &quot;ğŸ˜Š&quot;     &quot;ğŸ˜&quot;     &quot;ğŸ˜&quot;     &quot;ğŸ˜‘&quot;     &quot;ğŸ˜”&quot;     &quot;ğŸ˜•&quot;    
 [85] &quot;ğŸ˜˜&quot;     &quot;ğŸ˜š&quot;     &quot;ğŸ˜&quot;     &quot;ğŸ˜ &quot;     &quot;ğŸ˜©&quot;     &quot;ğŸ˜¬&quot;     &quot;ğŸ˜­&quot;    
 [92] &quot;ğŸ˜±&quot;     &quot;ğŸ˜³&quot;     &quot;ğŸ™ƒ&quot;     &quot;ğŸ™„&quot;     &quot;ğŸ™Œ&quot;     &quot;ğŸ™&quot;     &quot;ğŸš€&quot;    
 [99] &quot;ğŸš£&quot;     &quot;ï¿¼&quot;      &quot;$&quot;      &quot;Â£&quot;      &quot;0&quot;      &quot;1&quot;      &quot;2&quot;     
[106] &quot;3&quot;      &quot;4&quot;      &quot;5&quot;      &quot;6&quot;      &quot;7&quot;      &quot;8&quot;      &quot;9&quot;     
[113] &quot;a&quot;      &quot;Ã¡&quot;      &quot;b&quot;      &quot;c&quot;      &quot;d&quot;      &quot;e&quot;      &quot;Ã©&quot;     
[120] &quot;f&quot;      &quot;g&quot;      &quot;h&quot;      &quot;i&quot;      &quot;i&quot;      &quot;Ã­&quot;      &quot;j&quot;     
[127] &quot;k&quot;      &quot;l&quot;      &quot;m&quot;      &quot;n&quot;      &quot;Ã±&quot;      &quot;o&quot;      &quot;Ã³&quot;     
[134] &quot;p&quot;      &quot;q&quot;      &quot;r&quot;      &quot;s&quot;      &quot;t&quot;      &quot;â„¢&quot;      &quot;u&quot;     
[141] &quot;v&quot;      &quot;w&quot;      &quot;x&quot;      &quot;y&quot;      &quot;z&quot;      &quot;ãƒ„&quot;     &quot;\ue003&quot;
[148] &quot;\ue022&quot; &quot;\ue032&quot; &quot;\ue111&quot; &quot;\ue20c&quot; &quot;\ue23e&quot; &quot;\ue242&quot; &quot;\ue243&quot;
[155] &quot;\ue245&quot; &quot;\ue247&quot; &quot;\ue24a&quot; &quot;\ue24b&quot; &quot;\ue312&quot; &quot;\ue326&quot; &quot;\ue327&quot;
[162] &quot;\ue328&quot; &quot;\ue329&quot; &quot;\ue32d&quot; &quot;\ue32e&quot; &quot;\ue335&quot; &quot;\ue415&quot; &quot;\ue418&quot;
[169] &quot;\ue41a&quot; &quot;\ue41b&quot; &quot;\ue425&quot; &quot;\ue437&quot; &quot;ğŸ¤™&quot;     &quot;ğŸ¤©&quot;     &quot;ğŸ¤«&quot;    
[176] &quot;ğŸ¤·&quot;    </code></pre>
<p>Oh, ok. Sure. Thereâ€™s a whole heap of emojis. There are also a ton of characters that start with <code>\ue</code>: not sure yet what thatâ€™s about, or how that will affect the model. Perhaps weâ€™ll find out later. It seems fitting that the last character is this lady: ğŸ¤·.</p>
</div>
</div>
<div id="preparing-text-data-for-an-lstm-neural-network" class="section level1">
<h1>Preparing text data for an LSTM neural network</h1>
<p>First, I cut the text in semi-redundant sequences of <code>maxlen</code> characters. Iâ€™m using the average number of characters in a text as the maximum cut length. This splits the data up at a ton of (repeating) cut points, and separates each piece into two vectors: the sequence of <code>maxlen</code> characters, and the next character that comes after them.</p>
<pre class="r"><code>maxlen &lt;- messages %&gt;% 
  mutate(count = nchar(msg)) %&gt;% 
  summarise(avg = round(mean(count, na.rm = TRUE), 0)) %&gt;% 
  as.numeric()</code></pre>
<pre class="r"><code>dataset &lt;- map(
  seq(1, length(text) - maxlen - 1, by = 1), 
  ~list(sentence = text[.x:(.x + maxlen - 1)], next_char = text[.x + maxlen])
)</code></pre>
<p>Each piece of the list will look something like this:</p>
<pre class="r"><code>dataset[[200]]</code></pre>
<pre class="text"><code>$sentence
 [1] &quot;e&quot; &quot;r&quot; &quot; &quot; &quot;p&quot; &quot;o&quot; &quot;i&quot; &quot;n&quot; &quot;t&quot; &quot;s&quot; &quot;!&quot; &quot; &quot; &quot;y&quot; &quot;o&quot; &quot;u&quot; &quot; &quot; &quot;d&quot;
[17] &quot;e&quot; &quot;f&quot; &quot;i&quot; &quot;n&quot; &quot;i&quot; &quot;t&quot; &quot;e&quot; &quot;l&quot; &quot;y&quot; &quot; &quot; &quot;h&quot; &quot;a&quot; &quot;v&quot; &quot;e&quot;

$next_char
[1] &quot; &quot;</code></pre>
<p>Iâ€™ll then transpose it so that I have one section: a list of <code>sentence</code> segments of 40 characters each, and a list of <code>next_char</code>s.</p>
<pre class="r"><code>dataset &lt;- transpose(dataset)</code></pre>
<p>Now, I have to vectorize the data. First Iâ€™ll initialize two arrays - one where Iâ€™ll provide the sentence data, and one where Iâ€™ll predict the next character. There will be a row for each item in both of the lists - in my dataset, thatâ€™s 99101 rows of (sentence chunk) + (next-char) pairs.</p>
<p>The â€œnext charâ€ array is just a matrix. There is one column for every character in my character list training set - 175, since this list includes letters, digits, punctuation marks, and even emojis and other unicodes. If the given character is the â€œnext characterâ€ for a given item in our next char list, that box will have a 0, if not, itâ€™s a 0.</p>
<p>The sentence array acts the same way, but it has a third dimension of <code>maxlen</code> length - one position for each character in each of the sentence chunks.</p>
<pre class="r"><code>x &lt;- array(0, dim = c(length(dataset$sentence), maxlen, length(chars)))
y &lt;- array(0, dim = c(length(dataset$sentence), length(chars)))

for(i in 1:length(dataset$sentence)){
  
  x[i,,] &lt;- sapply(chars, function(x){
    as.integer(x == dataset$sentence[[i]])
  })
  
  y[i,] &lt;- as.integer(chars == dataset$next_char[[i]])
  
}</code></pre>
</div>
<div id="defining-a-keras-model" class="section level1">
<h1>Defining a keras model</h1>
<p>Finally, itâ€™s time to define our model.</p>
<p>As we add the layers (e.g. <code>layer_lstm</code>, to add a long short-term memory unit), we have to pass them several arguments:</p>
<ol style="list-style-type: decimal">
<li>object: this is the model object.</li>
<li>units: in an LSTM, this is the number of hidden memory units that the model holds on to when making a prediction. More units tends to lead to better predictions, but they take up a lot of RAM and a lot of time.</li>
<li>input_shape. <em>â€œDimensionality of the input (integer) not including the samples axis. This argument is required when using this layer as the first layer in a model.â€</em> This is just the dimensions of our training data, not including the samples axis (The length of each list in the <code>dataset</code> object). Here, Iâ€™ve asked for 37 characters per training chunk, and I have 176 unique character possibilities, so the input shape is c(37, 176).</li>
</ol>
<p>The densely-connected neural network layer is added with <code>layer_dense.</code> Here we only have to specify the units of the output space. Here, this is our 125 unique possible character outputs. Finally, we add the softmax function to get a probability distribution over these 125 different possible next characters.</p>
<p>Here weâ€™re using a Root Mean Square Propagation optimizer and a categorical cross entropy loss function. You can get into the weeds over these choices, but the point is that theyâ€™re good standard choices for predicting multiple (but mutually exclusive) classes, like characters in a word.</p>
<pre class="r"><code>model &lt;- keras_model_sequential()

model %&gt;%
  layer_lstm(128, input_shape = c(37, 176)) %&gt;%
  layer_dense(176) %&gt;% 
  layer_activation(&quot;softmax&quot;)

optimizer &lt;- optimizer_rmsprop(lr = 0.01)

model %&gt;% compile( 
  loss = &quot;categorical_crossentropy&quot;, 
  optimizer = optimizer
)</code></pre>
</div>
<div id="defining-model-output" class="section level1">
<h1>Defining model output</h1>
<p>As I train the model, I want it to periodically show me its predictions. We can do this by defining the <code>callback</code> that happens <code>on_epoch_end</code>. After every epoch, I want this to predict a string of characters. The length of that string is defined when we <code>fit()</code> the model.</p>
<p>I also want to vary the kinds of predictions I get by varying the <code>diversity</code> parameter. That does what it sounds like it will, and it will become <strong>very</strong> clear what that parameter is doing when we start looking at output.</p>
<pre class="r"><code>sample_mod &lt;- function(preds, temperature = 1){
  preds &lt;- log(preds)/temperature
  exp_preds &lt;- exp(preds)
  preds &lt;- exp_preds/sum(exp(preds))
  
  rmultinom(1, 1, preds) %&gt;% 
    as.integer() %&gt;%
    which.max()
}

on_epoch_end &lt;- function(epoch, logs) {
  
  cat(sprintf(&quot;epoch: %02d ---------------\n\n&quot;, epoch))
  
  for(diversity in c(0.2, 0.5, 1, 1.2)){
    
    cat(sprintf(&quot;diversity: %f ---------------\n\n&quot;, diversity))
    
    start_index &lt;- sample(1:(length(text) - maxlen), size = 1)
    sentence &lt;- text[start_index:(start_index + maxlen - 1)]
    generated &lt;- &quot;&quot;
    
    for(i in 1:400){
      
      x &lt;- sapply(chars, function(x){
        as.integer(x == sentence)
      })
      x &lt;- array_reshape(x, c(1, dim(x)))
      
      preds &lt;- predict(model, x)
      next_index &lt;- sample_mod(preds, diversity)
      next_char &lt;- chars[next_index]
      
      generated &lt;- str_c(generated, next_char, collapse = &quot;&quot;)
      sentence &lt;- c(sentence[-1], next_char)
      
    }
    
    cat(generated)
    cat(&quot;\n\n&quot;)
    
  }
}

print_callback &lt;- callback_lambda(on_epoch_end = on_epoch_end)</code></pre>
</div>
<div id="training-the-model" class="section level1">
<h1>Training the model</h1>
<p>Finally, itâ€™s time to train the model. Itâ€™s important to remember that <code>%&gt;%</code> works differently on keras models than it does in other R packages. Typically, to change an existing object, we have to pass a function to it using &lt;- or = like so:</p>
<pre class="r"><code>dataset &lt;- dataset %&gt;% 
  mutate(newvar = oldvar + 3)</code></pre>
<p><strong>This is NOT the case in Keras models.</strong> Keras models are <strong>modified in place</strong>, which means that everytime you pipe a model into a set of arguments, it gets updated. <code>model %&gt;%</code>, NOT <code>model &lt;- model %&gt;%</code>.</p>
<p>So, before we get started, hereâ€™s where we are if we just print out the model to the console.</p>
<pre class="r"><code>model</code></pre>
<pre><code>## Model
## ___________________________________________________________________________
## Layer (type)                     Output Shape                  Param #     
## ===========================================================================
## lstm_1 (LSTM)                    (None, 128)                   156160      
## ___________________________________________________________________________
## dense_1 (Dense)                  (None, 176)                   22704       
## ___________________________________________________________________________
## activation_1 (Activation)        (None, 176)                   0           
## ===========================================================================
## Total params: 178,864
## Trainable params: 178,864
## Non-trainable params: 0
## ___________________________________________________________________________</code></pre>
<p>Iâ€™m using a <strong>batch size</strong> of 128, which is the number of training examples in one forward/backward pass. An <strong>epoch</strong> is one forward pass and one backward pass of all the training examples. The higher the batch size, the more memory space youâ€™ll need.</p>
<pre class="r"><code>model %&gt;% fit(
  x, y,
  batch_size = 128,
  epochs = 5,
  callbacks = print_callback
)</code></pre>
<p>I played around with different parameters for a while, but kept forgetting how many epochs Iâ€™d trained on different models. Eventually, I cleared everything from memory, turned on 50 epochs, and went to bed with my computer running.</p>
<p>When I woke up, I had 200 different computer guesses of a conversation between two people who would eventually get engaged.</p>
</div>
<div id="lstm-text-output" class="section level1">
<h1>LSTM text output</h1>
<p>Because of the <code>print_callback</code> that we defined, I can check the loss and the predictions for 50 epochs with 4 different levels of diversity each. (Only roughly 20 would print to my console at any one time, but because I ran this from within an <code>.Rmd</code>, I could see all the predictions from all 50 epochs.</p>
<p>Letâ€™s check in on the first guess. This model was never going to give good predictions after only one epoch, but I wanted to stop here to see what this thing was putting out. Itâ€™s first attempt, with a 0.2 diversity, wasâ€¦ not great.</p>
<pre class="text"><code>Epoch 1/50
272957/272957 [==============================] - 582s 2ms/step - loss: 2.0566
epoch: 00 ---------------

diversity: 0.200000 ---------------

ahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahah</code></pre>
<p>This isâ€¦a rude guess. This isnâ€™t <em>too</em> far off from the kind of text I am prone to send, but I donâ€™t think I have the patience to go on for 400 characters. Clearly, the model has gotten stuck in a feedback loop.</p>
<p>When we increase the diversity, we get more punctuation and new lines - indication that the model is freeing itself up to choose from less common characters. However, this is done at the expense of English words.</p>
<p>(Also: from here on, Iâ€™ve entered all of the most interesting model outputs into an <a href="iphonefaketext.com/">fake iPhone text generator</a> to make it feel more real. I arbitrarily varied who was sending and who was receiving, because the output doesnâ€™t tell me that, and chose breaks that <em>~</em> felt right <em>~</em>, but I havenâ€™t changed any of the text. I <em>really</em> couldnâ€™t make this up.)</p>
<div class="figure">
<img src="/figs/2018-03-07-neural-nets-in-R/epoch1-1.2.png" />

</div>
<p>Yikes. Here we have the opposite problem - plenty of different characters, but itâ€™s not frequently landing on real words.</p>
<p>Our Goldilocks (not too hot, not too cold) might be around 0.5 - at least for this epoch.</p>
<div class="figure">
<img src="/figs/2018-03-07-neural-nets-in-R/epoch1-0.5.png" />

</div>
<p>Not bad for the first round. Because I havenâ€™t made the training data public, youâ€™re just going to have to take my word for it that Iâ€™ve never asked anyone to cruttion with me, nor have I been to the Med the Camece.</p>
</div>
<div id="measuring-performance" class="section level1">
<h1>Measuring performance</h1>
<p>Of course, the point of running several epochs is to improve upon the model. In the first round, our loss was 2.0566, but it gets lower as we go through For example, hereâ€™s an excerpt from epoch 12, with the diversity set at 1.0.</p>
<p>We got down to a loss of 1.5461, which isnâ€™t a huge change from the starting point. Iâ€™m mostly interpreting the outputs of the loss function in relation to one another, so this feels like an improvement.</p>
<div class="figure">
<img src="/figs/2018-03-07-neural-nets-in-R/epoch-12-1.png" />

</div>
<p>This, strucutrally, looks much more like a casual text exchange. Already, we see that some texts bits start with â€œhi!â€, punctuation comes at reasonable places, and we split some sentences over multiple texts. (It even spelled yâ€™all right, which some of yâ€™all still seem to be confused about.)</p>
<p>Itâ€™s also generating some #hip #new #slang that Iâ€™m excited to use in real life. â€œDemat, girl, we workin!â€ This feels like a 2018 version of trying to make Fetch happen.</p>
<p>So the model seems to be improving. Then, an odd thing happens. The loss function metric starts shooting up dramatically, and output gets more and more wild and uninterpretable. At the 12th epoch we find a local minimum, but then things start to unravel.</p>
<p>At epoch 16, weâ€™re back up to a loss of 1.837, and the model seems to have â€œforgottenâ€ some rules itâ€™s learned about which characters are usually next to one another, where apostrophes go, and how long words are allowed to be.</p>
<div class="figure">
<img src="/figs/2018-03-07-neural-nets-in-R/epoch-16.png" />

</div>
<p>By epoch 30, the model has falling completely out of its tree, departing entirely from any semblance of human language, English or otherwise.</p>
<div class="figure">
<img src="/figs/2018-03-07-neural-nets-in-R/epoch-30.png" />

</div>
<div id="whats-happening-here" class="section level2">
<h2>Whatâ€™s happening here?</h2>
<p>So whatâ€™s going on? We were doing so well, making slow and steady progress towards English sentences, and then slid right off the rails. Why?</p>
<p>Broadly, itâ€™s because I donâ€™t have enough training data to run 50 epochs of this model with these parameters. As we run through the epochs, the model learns and reinforces the patterns that it sees. In the beginning, this improves its performance. As we start to run out of data, though, the model runs out of patterns to find, and starts fixating on details that arenâ€™t actually important to generating human language. Remember, this is a completely vanilla model when we start. How should it know about grammar?</p>
</div>
</div>
<div id="iterating" class="section level1">
<h1>Iterating</h1>
<p>I tried a few more rounds, varying the <code>maxlen</code> input sequence, the learning rate of the optimizer (<code>optimizer_rmsprop(lr = 0.01)</code>).</p>
<p>In keeping with the timeless paradigm â€œgarbage in, garbage outâ€, I also returned to the input data to clean it a bit more. Remember all those characters that started with <code>\ue</code>? I found the culprit.</p>
<pre class="r"><code>ue = purrr::map(chars[147:172], ~which(str_detect(messages$msg, .x))) %&gt;%
  unlist() 

messages$msg[min(ue-1):max(ue+1)]</code></pre>
<pre class="text"><code>[1] &quot;Haha, I couldn&#39;t figure out how to get the emoji keyboard on this thing, I think I downloaded the wrong app or something, but it has some pheNOMenal artwork&quot;
[2] &quot;â”Œî€¢ã€€   â”Œâ” â”‚â”‚ã€€ã€€â”‚â”‚ î€¢â””â”€â”€î€¢îŒµ â””â”€â”â”Œâ”€â”˜ îŒµ   â””î€¢ â”Œî€¢â”€â”€â”€îŒµ â”‚ã€€ã€€ã€€ã€€â”‚ â”‚     î€¢â”â”‚ î€¢ã€€  â””â”˜â”‚ â””â”€â”€â”€â”€î€¢ â”Œâ”ã€€ã€€â”Œî€¢ã€€ â”‚â”‚ã€€ã€€â”‚â”‚ î€¢îŒµã€€  â”‚â”‚ â”‚â””â”€â”€î€¢â”‚ â””â”€â”€î€¢â”€îŒµ î•ARE MINEî•&quot;                   
[3] &quot;I think they&#39;re all taken directly from Cher tweets&quot;                                                                                                         
[4] &quot;îˆŒîˆŒîˆŒ          îˆŒîˆŒîˆŒ îˆŒî‰Šî‰‹îˆŒ    îˆŒî‰…î‰‚ îˆŒ îˆŒî‰‚       î‰Šî‰…îˆŒî‰‚î‰…î‰ŠîˆŒ îˆŒî‰ƒî‰…îŒ­LoveîŒ­î‰ƒîˆŒ     îˆŒî‰‡CHARMî‰ƒîˆŒ         îˆŒî‰‚î‰… î‰‚îˆŒ              îˆŒîˆ¾îˆŒ                   îˆŒ&quot;                               
[5] &quot;îŒ® /â–  â–  â– \\~îŒ¦     |  î€¢ î€¢   |    îŒ®     |     îš     î› îŒ® \\ï¼¿î€ƒï¼¿/ *I&#39;m Fallin in LOVE* îŒ©îŒ§î„‘îŒ’î¥î·îŒ¨ ILuvYOUî€²xoxoî˜&quot;                                                    
[6] &quot;This is someone&#39;s job. Or unpaid endeavor borne of passion. I can&#39;t tell which is more inspiring&quot;                                                            </code></pre>
<p>Haha, ok. Apparently those are just wild characters that at some point in their winding journey from A Weird App -&gt; Viber -&gt; Excel -&gt; R got mistranslated into a strange encoding. Iâ€™m just going to remove those lines from my input dataset.</p>
<pre class="r"><code>range(ue)

messages &lt;- messages[-(min(ue):max(ue)) , ]  </code></pre>
<p>With a heavy heart, Iâ€™m also going to test what happens if I remove all the emojis. ğŸ˜­</p>
<p>I donâ€™t ever use emojis as replacement for words in the middle of a sentence, but I sometimes use them for emphasis or confirmation. Because of this, I feel pretty confident that the meanings of sentences wonâ€™t change without them. Itâ€™s possible that having a smaller number of characters to choose from will help prevent the model from falling into an emoji-fueled feedback loop.</p>
<pre class="r"><code>text &lt;- messages$msg[is.na(messages$msg) == FALSE] %&gt;%  
  str_to_lower() %&gt;% 
  str_c(collapse = &quot;\n&quot;) %&gt;%  
  tokenize_characters(strip_non_alphanum = FALSE, simplify = TRUE)  
  
# converting emojis to NAs  
text &lt;- gsub(&#39;\\p{So}|\\p{Cn}&#39;,  
                       NA, 
                       text, perl = TRUE)

# removing the NAs formerly known as emojis 
text &lt;- text[!is.na(text)]
 
paste(&quot;Number of characters:&quot;, length(text))

chars &lt;- text %&gt;%
  unique() %&gt;%
  sort()
paste(&quot;total chars:&quot;, length(chars))</code></pre>
<pre class="text"><code>[1] &quot;Number of characters: 264891&quot;
[1] &quot;total chars: 82&quot;</code></pre>
<p>Without dramatically reducing the input data size, weâ€™ve more than halved the number of possible characters to draw from. This feels like progress! This is the reason that we also remove capital letters - itâ€™s likely that the model could learn that the first word of every sentence is capitalized, but the increased size of the pool of potential inputs isnâ€™t worth it for datasets of this size.</p>
<p>Of course, I had to regenerate x and y, and run the model again.</p>
<pre class="r"><code>model %&gt;% fit(
  x, y,
  batch_size = 150,
  epochs = 10,
  callbacks = print_callback
)</code></pre>
<p>These alterations may have helped slightly - we got loss down to 1.486, and it didnâ€™t shoot back up - but they didnâ€™t change the results materially.</p>
</div>
<div id="archetypes" class="section level1">
<h1>Archetypes</h1>
<p>In combing through the output, some patterns emerged. Here are some archetypes that show up a lot:</p>
<p><strong>1. The feedback loop</strong></p>
<p>With low diversity, the model OFTEN gets stuck in feedback loops. When trained on (many long-distance) texts between my fiancÃ© and myself, this often makes us come off WAY cornier than we really are.</p>
<div class="figure">
<img src="/figs/2018-03-07-neural-nets-in-R/corny.png" />

</div>
<p><strong>2. The alien conversation</strong></p>
<p>With high diversity, thereâ€™s clearly a conversation happening, but itâ€™s not in any language Iâ€™ve ever seen.</p>
<div class="figure">
<img src="/figs/2018-03-07-neural-nets-in-R/nonsense.png" />

</div>
<p><strong>3. The fake deep poetry</strong></p>
<p>This is self explanatory. In the wise words of <a href="https://www.youtube.com/watch?v=0-3ctfy1M2s">Leslie Knope</a>:</p>
<p>Anything / Can Be a Slam / Poem / If you Say it like This</p>
<div class="figure">
<img src="/figs/2018-03-07-neural-nets-in-R/fake-deep.png" />

</div>
<p><strong>4. The <a href="https://www.poetryfoundation.org/poems/42916/jabberwocky">Jabberwock</a></strong></p>
<p>Much of this output, unsurprisingly, is Jabberwocky. If you read it in a Scottish accent, it could also pass for a Robbie Burns poem. The structure is mostly intuitive, but there are too many vowels and consonants in all the wrong places. It doesnâ€™t have to make any sense for it to sound nice.</p>
<center>
<video controls="controls" width="300" height="600" src="/figs/2018-03-07-neural-nets-in-R/IMG_0930.MOV">
</video>
</center>
</div>
<div id="takeaways" class="section level1">
<h1>Takeaways</h1>
<p>In summary, this is the first of the start. The thing in the time. I think. I think. I donâ€™t think. I might be the world?</p>
</div>
